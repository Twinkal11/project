# -*- coding: utf-8 -*-
"""curated_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l8k_UXB93tCAPn6dmpe3HJN7wmk5uA-Y
"""

!pip3 install pyspark

import pyspark
from pyspark.context import SparkContext
from pyspark.sql.context import SQLContext
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import *
import re

from pyspark.sql.functions import desc

class curated_layer():
  spark = SparkSession.builder.appName("Project-Stage-II").config('spark.ui.port', '4050')\
        .config("spark.master", "local").enableHiveSupport().getOrCreate()
  curated = spark.read.csv("/content/drive/MyDrive/clean_layer/clean_layer.csv",header="True")
  
  curated.groupBy("method").count().orderBy(desc("count")).show()
  
  curated.select(count(curated.method)).show()
  
  clean_df = curated.withColumn('Hour', hour('timestamp'))
  
  # clean_df.groupBy("host").count().orderBy(desc("count"))
  
 

  # device_pattern = r'(Mozilla|Dalvik|Goog|torob|Bar).\d\S+\s\((\S\w+;?\s\S+(\s\d\.(\d\.)?\d)?)'
  # device_curated=clean_df.withColumn('device', regexp_extract(col('User_agent'), device_pattern, 2))
  # device_curated.show()


  def curated_l(self):

    print("add column hour,Get,Post,Head")
    self.curated_lyr =self.clean_df.withColumn("No_get", when(col("method") == "GET", "GET")) \
            .withColumn("No_post", when(col("method") == "POST", "POST")) \
            .withColumn("No_Head", when(col("method") == "HEAD", "HEAD"))\
            # .withColumn("date","date")
    # self.curated_lyr.show(100)

  def per_device(self):
      print("agg_per_ip")
        # # perform aggregation per device
      self.agg_per_ip = self.curated_lyr.select("*").groupBy("date","client/ip","Hour").agg(
            count("Row_id").alias("row_id"),
            count(col("No_get")).alias("no_get"), count(col("No_post")).alias("no_post"),
            count(col("No_Head")).alias("no_head"))

      self.agg_per_ip.show() 



  def across_device(self):
      print("agg_across_device")
      agg_across_device = self.curated_lyr.select("*").groupBy("date","Hour").agg(
                                                   
                                                    count("client/ip").alias("count_client_ip"),
                                                    count(col("No_get")).alias("no_get"),
                                                    count(col("No_post")).alias("no_post"),
                                                    count(col("No_head")).alias("no_head"))







      agg_across_device.show()

  def cur_save(self):
    self.clean_df.write.csv("/content/drive/MyDrive/curated_layer/", header=True,mode='overwrite')

  def hive_table_for_per_ip(self):
    pass
    self.agg_per_ip.write.option("mode","overwrite").saveAsTable('per_ipp')
    self.spark.sql("select count(*) from per_ipp").show()
  
  
   

if __name__ == '__main__':
    cur = curated_layer()
    cur.curated_l()
    cur.per_device()
    cur.across_device()  
    cur. cur_save()
    cur.hive_table_for_per_ip()



