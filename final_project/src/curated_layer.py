# -*- coding: utf-8 -*-
"""curated_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l8k_UXB93tCAPn6dmpe3HJN7wmk5uA-Y
"""

!pip3 install pyspark

import pyspark
from pyspark.context import SparkContext
from pyspark.sql.context import SQLContext
from pyspark.sql.session import SparkSession
from pyspark.sql.functions import *
import re

from pyspark.sql.functions import desc
 
from pyspark.sql import HiveContext

class curated_layer():
  spark = SparkSession.builder.appName("Project-Stage-II").config('spark.ui.port', '4050')\
        .config("spark.master", "local").enableHiveSupport().getOrCreate()
  curated = spark.read.csv("/content/drive/MyDrive/clean_layer/clean_layer.csv",header="True")
  curated.show(100)
  
  curated.groupBy("method").count().orderBy(desc("count"))
  
  
  def curated_l(self):

    print("add column hour,Get,Post,Head")
    self.curated =self.curated.withColumn("No_get", when(col("method") == "GET", "GET")) \
            .withColumn("No_post", when(col("method") == "POST", "POST")) \
            .withColumn("No_Head", when(col("method") == "HEAD", "HEAD")) \
            .withColumn("day", to_date(col("Datetime"))) \
            .withColumn("hour", hour(col("Datetime"))) \
            .withColumn("day_hour", concat(col("day"), lit(" "), col("hour")))
            

    

  def per_device(self):
      print("agg_per_ip")
        # # perform aggregation per device
      self.agg_per_ip = self.curated.select("Row_id", "day_hour", "client/ip", "no_get", "no_post", "no_head") \
            .groupBy("day_hour", "client/ip") \
            .agg(count("Row_id").alias("count_of_records"),
                 count(col("No_get")).alias("no_get"),
                 count(col("No_post")).alias("no_post"),
                 count(col("No_head")).alias("no_head"))
      self.agg_per_ip.show(100) 



  def across_device(self):
      print("agg_across")
      self.agg_across = self.curated.select("*") \
            .groupBy("day_hour") \
            .agg(
                count("client/ip").alias("no_of_clients"),
                count(col("No_get")).alias("no_get"),
                count(col("No_post")).alias("no_post"),
                count(col("No_head")).alias("no_head")
                )







      self.agg_across.show(100)

  def curated_layer(self):
    self.curated.write.csv("/content/drive/MyDrive/curated_layer", header=True,mode='overwrite')

  def cur_save(self):
    self.agg_per_ip.write.csv("/content/drive/MyDrive/curated_layer/agg_per_ip/", header=True,mode='overwrite')

  def across_save(self):  
    self.agg_across.write.csv("/content/drive/MyDrive/curated_layer/agg_across_/", header=True,mode='overwrite')

  def hive_table_for_per_ip(self):
    pass
    sqlContext = HiveContext(self.spark.sparkContext)
    sqlContext.sql('DROP TABLE IF EXISTS agg_per_ip')
    self.agg_per_ip.write.option("mode","overwrite").saveAsTable('agg_per_ip')
    self.spark.sql("select count(*) from agg_per_ip").show()
  
  
   

if __name__ == '__main__':
    cur = curated_layer()
    cur.curated_l()
    cur.per_device()
    cur.across_device()  
    cur. cur_save()
    cur.across_save()
    cur.hive_table_for_per_ip()



